{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7b96e0c-c92e-4094-a01c-e98292a9f578",
   "metadata": {},
   "source": [
    "### Theoretical Questions\n",
    "\n",
    "#### 1. Overfitting and Underfitting\n",
    "- **Overfitting:** Overfitting occurs when a machine learning model captures noise in the training data, rather than the underlying distribution. It performs very well on training data but poorly on unseen data. Overfitting typically happens when the model is too complex relative to the amount and variability of the data.\n",
    "- **Underfitting:** Underfitting occurs when a model is too simple to capture the underlying structure of the data. It performs poorly on both training and unseen data. Underfitting typically happens when the model is too simple relative to the complexity of the data.\n",
    "- **Techniques to address these issues:**\n",
    "  - **Overfitting:** Use cross-validation, prune decision trees, apply regularization techniques (like Lasso, Ridge), reduce the complexity of the model, increase the amount of training data, or use dropout in neural networks.\n",
    "  - **Underfitting:** Increase the model complexity, add more features, reduce regularization, or use more sophisticated algorithms.\n",
    "\n",
    "#### 2. Bias-Variance Tradeoff\n",
    "- **Bias:** Bias is the error introduced by approximating a real-world problem, which may be complex, by a much simpler model. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting).\n",
    "- **Variance:** Variance is the error introduced by the model's sensitivity to small fluctuations in the training set. High variance can cause an algorithm to model the noise in the training data, rather than the intended outputs (overfitting).\n",
    "- **Tradeoff:** The bias-variance tradeoff is the balance between these two sources of error that must be managed to build models that generalize well. Increasing the complexity of the model typically reduces bias but increases variance, and vice versa.\n",
    "\n",
    "#### 3. Gradient Descent\n",
    "- **How it works:** Gradient Descent is an optimization algorithm used to minimize the cost function in machine learning models. It iteratively adjusts the parameters of the model by moving in the direction of the negative gradient of the cost function.\n",
    "- **Variants:**\n",
    "  - **Batch Gradient Descent:** Uses the entire dataset to compute the gradient. It can be slow and computationally expensive for large datasets.\n",
    "  - **Stochastic Gradient Descent (SGD):** Uses one data point at a time to compute the gradient. It's faster and can handle large datasets but introduces more noise in the gradient estimation.\n",
    "  - **Mini-batch Gradient Descent:** Uses a small random subset of data to compute the gradient. It balances the efficiency of Batch Gradient Descent and the noise reduction of SGD.\n",
    "- **Usage:** Use Batch Gradient Descent for smaller datasets, SGD for large and streaming datasets, and Mini-batch Gradient Descent for a compromise between the two."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8aa27c-8e16-41e2-b819-5507b2b3fb3f",
   "metadata": {},
   "source": [
    "### Data Manipulation Tasks\n",
    "\n",
    "####  Pandas Data Manipulation\n",
    "Let's start with the given CSV file (`Sample_Sales_Data.csv`).\n",
    "\n",
    "**Tasks:**\n",
    "- **Load the data into a Pandas DataFrame.**\n",
    "- **Calculate the total sales for each product category.**\n",
    "- **Find the top 5 products with the highest sales.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9499b148-cecb-4f80-8606-b40694bfb305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sales by category:\n",
      "ProductCategory\n",
      "Clothing         6800\n",
      "Electronics    186000\n",
      "Furniture       21250\n",
      "Name: TotalSales, dtype: int64\n",
      "\n",
      "Top 5 products with the highest sales:\n",
      "ProductName\n",
      "Smartphone    120000\n",
      "Laptop         50000\n",
      "Sofa           10000\n",
      "Monitor         9000\n",
      "Table           9000\n",
      "Name: TotalSales, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data into a Pandas DataFrame\n",
    "df = pd.read_csv('Sample_Sales_Data.csv')\n",
    "\n",
    "# Calculate the total sales for each product category\n",
    "total_sales_by_category = df.groupby('ProductCategory')['TotalSales'].sum()\n",
    "\n",
    "# Find the top 5 products with the highest sales\n",
    "top_5_products = df.groupby('ProductName')['TotalSales'].sum().nlargest(5)\n",
    "\n",
    "# Output the results\n",
    "print(\"Total sales by category:\")\n",
    "print(total_sales_by_category)\n",
    "print(\"\\nTop 5 products with the highest sales:\")\n",
    "print(top_5_products)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d91e8ac-cf70-4418-9dac-15e9bb86c603",
   "metadata": {},
   "source": [
    "####  Numpy Array Operations\n",
    "Let's create a 3x3 NumPy array with random integers and perform the specified operations.\n",
    "\n",
    "**Tasks:**\n",
    "- **Compute the sum of all elements in the array.**\n",
    "- **Find the mean and standard deviation of the elements.**\n",
    "- **Normalize the array (subtract the mean and divide by the standard deviation).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03b28461-f0e2-4666-b786-f50eb59d7784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array:\n",
      "[[15 36 15]\n",
      " [41 22 21]\n",
      " [95 38 12]]\n",
      "\n",
      "Sum of all elements: 295\n",
      "Mean of elements: 32.77777777777778\n",
      "Standard deviation of elements: 24.256855973691128\n",
      "\n",
      "Normalized array:\n",
      "[[-0.73289703  0.13283759 -0.73289703]\n",
      " [ 0.33896488 -0.44431883 -0.48554428]\n",
      " [ 2.56513962  0.2152885  -0.85657341]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a 3x3 NumPy array with random integers\n",
    "array = np.random.randint(1, 100, size=(3, 3))\n",
    "\n",
    "# Compute the sum of all elements in the array\n",
    "sum_of_elements = np.sum(array)\n",
    "\n",
    "# Find the mean and standard deviation of the elements\n",
    "mean = np.mean(array)\n",
    "std_dev = np.std(array)\n",
    "\n",
    "# Normalize the array\n",
    "normalized_array = (array - mean) / std_dev\n",
    "\n",
    "# Output the results\n",
    "print(\"Array:\")\n",
    "print(array)\n",
    "print(\"\\nSum of all elements:\", sum_of_elements)\n",
    "print(\"Mean of elements:\", mean)\n",
    "print(\"Standard deviation of elements:\", std_dev)\n",
    "print(\"\\nNormalized array:\")\n",
    "print(normalized_array)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
